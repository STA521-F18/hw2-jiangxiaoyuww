---
title: "HW2 STA521 Fall18"
author: "Billy Jiang xj35 jiangxiaoyuww"
date: "Due September 23, 2018 5pm"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

## Backgound Reading

Readings: Chapters 3-4 in Weisberg Applied Linear Regression


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


## Exploratory Data Analysis

0.  Preliminary read in the data.  After testing, modify the code chunk so that output, messages and warnings are suppressed.  

```{r data}
library(alr3)
data(UN3, package="alr3")
library(car)
library("knitr")
```


1. Create a summary of the data.  How many variables have missing data?  Which are quantitative and which are qualtitative?

```{r}
summary(UN3)
missingvalue = sapply(UN3, function(x) sum(is.na(x)))
mode = sapply(UN3, function(x) class(x))
f = rbind(missingvalue,mode)
row.names(f) = c('missing_value', 'mode')
kable(f)
```

Ans: From our data, there are six variables with missing data. It seems that all of them are quantitive.

2. What is the mean and standard deviation of each quantitative predictor?  Provide in a nicely formatted table.

```{r}
tempm = sapply(UN3, function(x) mean(x, na.rm = TRUE))
tempsd = sapply(UN3, function(x) sd(x, na.rm = TRUE))
total = rbind(tempm,tempsd)
rownames(total) = c("mean", "sd")
total = t(total)
kable(total)
```


3. Investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings regarding trying to predict `ModernC` from the other variables.  Are there potential outliers, nonlinear relationships or transformations that appear to be needed based on your graphical EDA?

```{r}
library(GGally)
UN3final = UN3[complete.cases(UN3),]
ggpairs(UN3final)
```

Ans: Countries with one or more missing variables are excluded because they are not included in our final model. From our plots, we see that PPgdp, Pop and Purban are strongly and positively correlated with MordenC. Fertility and Change, on the other hand, are negatively correlated with MordenC. There are seem to have two very influential points for Pop. The relationship between PPgdp and ModernC, Fertility and ModernC seem to be nonlinear in some way, one way to solve this problem to log transform our variables.

## Model Fitting

4.  Use the `lm()` function to perform a multiple linear regression with `ModernC` as the response and all other variables as the predictors, using the formula `ModernC ~ .`, where the `.` includes all remaining variables in the dataframe.  Create  diagnostic residual plot from the linear model object and comment on results regarding assumptions.  How many observations are used in your model fitting?

```{r}
lmo = lm(ModernC~.,data = UN3)
summary(lmo)
par(mfrow=c(2,2))
plot(lmo)
```

Ans: With 85 observations deleted due to missingness, there are 125 observations used in the model fitting.The Normal Q-Q suggests that our sample is heavily tailed. Minor heteroscedastic trend is also present. From the fourth plot, we see taht China, India and Kuwait seem to be particularly influential. 

5. Examine added variable plots `car::avPlot` or `car::avPlots`  for your model above. Are there any plots that suggest that transformations are needed for any of the terms in the model? Describe. Is it likely that any of the localities are influential for any of the terms?  Which localities?  Which terms?  

```{r}
car::avPlots(lm(formula = ModernC ~ ., data = UN3))
```

Ans: From the plots presented, there are not any plots that suggest the strong need of transformation; however, it never hurts to compare with the log transformation graph. From pop|others plot, we see that China and India seem to be influential in determining the slope of the fitted line. Other points seem to be clustered. Thailand also might pull the slope a little bit up for the Fertility graph. From problem 3, we also see that PPgdp, Pop seem to suggest log transformations. 

6.  Using the Box-Tidwell  `car::boxTidwell` or graphical methods find appropriate transformations of the predictor variables to be used as predictors in the linear model.  If any predictors are negative, you may need to transform so that they are non-negative.  Describe your method and  the resulting transformations.


```{r}
library(alr3)
library(dplyr)
final = UN3[complete.cases(UN3), ]
car::boxTidwell(ModernC ~ PPgdp + Pop, other.x = ~Change + Fertility + Purban + Frate, data = final)
lmtransform = lm(formula = ModernC ~ Change + log(PPgdp) + Frate + log(Pop) + Fertility + Purban, final)
plot(lmtransform)
```

Ans: From our test, it seems that nothing should be transformed; however we log transform PPgdp and Pop from the nonlinearities we detected in problem 3. The residual plot after transformation seems better as variances became more constant and less influential points. In other literatures, PPgdp and Pop are often log transformed, which can serve as some sort of prior belief.

7. Given the selected transformations of the predictors, select a transformation of the response using `MASS::boxcox` or `car::boxCox` and justify.


```{r}
car::boxCox(lmtransform)
powerTransform(lmtransform)
```

Ans: From the boxCox plot, it seems that lambda is around 0.75, with the 95 CI covering 0.5 to 1. There is not much point in transforming our response variable, we can just choose lambda to be 1, which is within the CI.

8.  Fit the regression using the transformed variables.  Provide residual plots and added variables plots and comment.  If you feel that you need additional transformations of either the response or predictors, repeat any steps until you feel satisfied.
```{r}
summary(lmtransform)
par(mfrow=c(2,2))
plot(lmtransform)
car::avPlots(lm(formula = ModernC~Change + log(PPgdp) + Frate + log(Pop) + Fertility + Purban, data = UN3))
```

Ans: One of the major difference is we see that the significance value for log(PPgdp) went up compare to PPgdp, suggesting that log transformation of PPgdp fits better with ModernC. Also, we notice that the intercept is no longer significant, probably due to our transformed PPgdp. As for residual plot, the variance seems to be more consistent. Normal Q-Q suggest that points are closer to our theoretical line. Less influential points are also suggested by Leverage plot. The transformation gives results closer to our assumptions for OLS. 

9. Start by finding the best transformation of the response and then find transformations of the predictors.  Do you end up with a different model than in 8?


```{r}
car::boxCox(lm(formula = ModernC ~. , data = final))
car::boxTidwell(ModernC ~ PPgdp + Pop, other.x = ~Change + Fertility + Purban + Frate, data = final)

```

Ans: we see that the model is roughly the same in as in problem 8. It doesn't seem that a transformation of response is needed. Of course, if we choose not to transform the response variable, boxTidwell on predictors will give the same results. 

10.  Are there any outliers or influential points in the data?  Explain.  If so, refit the model after removing any outliers and comment on residual plots.


```{r}
lmtransform = lm(formula = ModernC ~ Change + log(PPgdp) + Frate + log(Pop) + Fertility + Purban, UN3)
par(mfrow=c(2,2))
plot(lmtransform)
car::avPlots(lmtransform)

```

Ans: Yeah, China and India seem to look like outliers at first; however, they don't seem as much after log transformation. From a qualitative perspective, China and India are also too important to remove from the data set.

## Summary of Results

11. For your final model, provide summaries of coefficients with 95% confidence intervals in a nice table with interpretations of each coefficient.  These should be in terms of the original units! 


```{r}
fit = lm(formula = ModernC ~ log(PPgdp) +  Frate + log(Pop) + Fertility, data = final)
anova(lmtransform,fit)
t = data.frame(confint(fit))
coef = coefficients(fit)
t = cbind(t,coef)
row.names(t) = c("Intercept","PPgdp","Frate","Pop","Fertility")
kable(t)

```

Ans: We choose to drop Purban and change as they are not significant in our previous models (confirmed by anova). PPgdp and Pop are log transformed; 95 CI suggests that if repeated samples are taken and 95% confidence inteval was computed for each sample, 95% of them would contain the true population mean. In this circumstance, we see that the intercept is between -31 and 25, so it gives virtually no information. The PPgdp,Frate and Pop all have positive coef and CI, suggesting an increase in these measures will increase Modern index by respective amount. Fertility has negaitive coefficients, suggesting it goes into the opposite direction of modernC. One way to explain log transformed predictor is that 10% increase in predictor will result in coef * log(1.1) increase in the response (given they move in the same direction).

12. Provide a paragraph summarizing your final model  and findings suitable for the US envoy to the UN after adjusting for outliers or influential points.   You should provide a justification for any case deletions in your final model



Ans: In my final model, I used log(PPgdp), logPop, Fertility and Frate to predict ModernC. Log transformations were employed to conform our data with the normal assumption and stay current with the existing literature. Response variable ModernC was not transformed, supported by boxCox test. Almost half of the countries are omitted due to missing variables in one or more categories; however, do note that China and India are two very influential points before the transformation, although they are too important to omit; Our model gives a pretty decent result, with Pop, PPgdp and Frate influence modernC positively. More population, high per capita GDP and more female economic participation are commonly associated with modernization, while people in poor countries which lack contraception methods have high fertility rate.


## Methodology

    
13. Prove that the intercept in the added variable scatter plot will always be zero.  _Hint:  use the fact that if $H$ is the project matrix which contains a column of ones, then $1_n^T (I - H) = 0$.  Use this to show that the sample mean of residuals will always be zero if there is an intercept._
$$
\begin{aligned}
H = X_j(X_jX_j^T)^{-1}X_j \\
e_Y &= Y - HY \\
&= (I-H)Y\\
e_X &= x_j - Hx_j \\
&= (I-H)x_j\\
e_Y = \beta_01_n +\beta_1e_x\\
(I - H)Y = \beta_01_n + (x_j^T(I-H)^T(I-H)x_j)^{-1}x_j^T(I-H)Y(I-H)x_j\\
x_j^T(I - H)Y = x_j^T\beta_0 + x_j^T(x_j^T(I-H)^T(I-H)x_j)^{-1}x_j^T(I-H)Y(I-H)x_j\\
x_j^T(I - H)Y = x_j^T\beta_0 + x_j^T(I-H)x_j(x_j^T(I-H)x_j)^{-1}x_j^T(I-H)Y\\
x_j^T(I - H)Y = x_j^T\beta_0 + x_j^T(I - H)Y\\
\beta_0 = 0
\cr
\frac{1}{n}\sum e_{i} = \frac{1}{n}1_n^{T}e_i = \frac{1}{n}1_n^T (I - H)Y = 0
\end{aligned}
$$
$X_j$ denotes feature matrix without jth feature and $x_j$ is the jth feature. 


14. For multiple regression with more than 2 predictors, say a full model given by `Y ~ X1 + X2 + ... Xp`   we create the added variable plot for variable `j` by regressing `Y` on all of the `X`'s except `Xj` to form `e_Y` and then regressing `Xj` on all of the other X's to form `e_X`.  Confirm that the slope in a manually constructed added variable plot for one of the predictors  in Ex. 10 is the same as the estimate from your model. 
```{r}
UN3final = UN3[complete.cases(UN3),]
e_Y = residuals(lm(ModernC ~  log(PPgdp) + Frate + log(Pop) + Fertility + Purban, UN3final)) #we omit change in feature matrix
lmtransformregressChange = lm(Change ~  log(PPgdp) + Frate + log(Pop) + Fertility + Purban, UN3final)
e_X = residuals(lmtransformregressChange)
lme = lm(e_Y ~ e_X)
lmtransform$coefficients["Change"]
lme$coefficients
```
Ans: We see that the coefficent of the Change is equal to the coefficient of the slope in our manually constructed added variable plot. 

